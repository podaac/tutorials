{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Before-you-start\" data-toc-modified-id=\"Before-you-start-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Before you start</a></span></li><li><span><a href=\"#Authentication-setup\" data-toc-modified-id=\"Authentication-setup-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Authentication setup</a></span></li><li><span><a href=\"#Hands-off-workflow\" data-toc-modified-id=\"Hands-off-workflow-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Hands-off workflow</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Access Sentinel-6 NRT Data\n",
    "\n",
    "This notebook shows a simple way to maintain a local time series of Sentinel-6 NRT data using the [CMR Search API](https://cmr.earthdata.nasa.gov/search/site/docs/search/api.html). It downloads granules the ingested since the previous run to a designated data folder and overwrites a hidden file inside with the timestamp of the CMR Search request on success.\n",
    "\n",
    "> **User note:**\n",
    ">  The notebook actually points to a MODIS SST collection for now ([https://doi.org/10.5067/GHMDA-2PJ19](https://doi.org/10.5067/GHMDA-2PJ19)). It will work just the same for Sentinel-6.\n",
    "\n",
    "## Before you start\n",
    "\n",
    "Before you beginning this tutorial, make sure you have an Earthdata account: [https://urs.earthdata.nasa.gov](https://urs.earthdata.nasa.gov) for the operations envionrment (most common) or [https://uat.urs.earthdata.nasa.gov](https://uat.urs.earthdata.nasa.gov) for the UAT environment.\n",
    "\n",
    "Accounts are free to create and take just a moment to set up.\n",
    "\n",
    "## Authentication setup\n",
    "\n",
    "We need some boilerplate up front to log in to Earthdata Login.  The function below will allow Python\n",
    "scripts to log into any Earthdata Login application programmatically.  To avoid being prompted for\n",
    "credentials every time you run and also allow clients such as curl to log in, you can add the following\n",
    "to a `.netrc` (`_netrc` on Windows) file in your home directory:\n",
    "\n",
    "```\n",
    "machine uat.urs.earthdata.nasa.gov\n",
    "    login <your username>\n",
    "    password <your password>\n",
    "```\n",
    "\n",
    "Make sure that this file is only readable by the current user or you will receive an error stating\n",
    "\"netrc access too permissive.\"\n",
    "\n",
    "`$ chmod 0600 ~/.netrc` \n",
    "\n",
    "*You'll need to authenticate using the netrc method when running from command line with [`papermill`](https://papermill.readthedocs.io/en/latest/). You can log in manually by executing the cell below when running in the notebook client in your browser.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-11T17:34:07.928944Z",
     "start_time": "2020-08-11T17:34:07.913616Z"
    }
   },
   "outputs": [],
   "source": [
    "from urllib import request\n",
    "from http.cookiejar import CookieJar\n",
    "import getpass\n",
    "import netrc\n",
    "\n",
    "\n",
    "def setup_earthdata_login_auth(endpoint):\n",
    "    \"\"\"\n",
    "    Set up the request library so that it authenticates against the given Earthdata Login\n",
    "    endpoint and is able to track cookies between requests.  This looks in the .netrc file \n",
    "    first and if no credentials are found, it prompts for them.\n",
    "\n",
    "    Valid endpoints include:\n",
    "        uat.urs.earthdata.nasa.gov - Earthdata Login UAT (Harmony's current default)\n",
    "        urs.earthdata.nasa.gov - Earthdata Login production\n",
    "    \"\"\"\n",
    "    try:\n",
    "        username, _, password = netrc.netrc().authenticators(endpoint)\n",
    "    except (FileNotFoundError, TypeError):\n",
    "        # FileNotFound = There's no .netrc file\n",
    "        # TypeError = The endpoint isn't in the netrc file, causing the above to try unpacking None\n",
    "        print('Please provide your Earthdata Login credentials to allow data access')\n",
    "        print('Your credentials will only be passed to %s and will not be exposed in Jupyter' % (endpoint))\n",
    "        username = input('Username:')\n",
    "        password = getpass.getpass()\n",
    "\n",
    "    manager = request.HTTPPasswordMgrWithDefaultRealm()\n",
    "    manager.add_password(None, endpoint, username, password)\n",
    "    auth = request.HTTPBasicAuthHandler(manager)\n",
    "\n",
    "    jar = CookieJar()\n",
    "    processor = request.HTTPCookieProcessor(jar)\n",
    "    opener = request.build_opener(auth, processor)\n",
    "    request.install_opener(opener)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-11T17:34:08.212835Z",
     "start_time": "2020-08-11T17:34:08.203168Z"
    }
   },
   "outputs": [],
   "source": [
    "setup_earthdata_login_auth('uat.urs.earthdata.nasa.gov')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hands-off workflow\n",
    "\n",
    "This workflow/notebook can be run routinely to maintain a time series of NRT data, downloading new granules as they become available in CMR. \n",
    "\n",
    "The notebook writes/overwrites a file `.update` to the target data directory with each successful run. The file tracks to date and time of the most recent update to the time series of NRT granules using a timestamp in the format `yyyy-mm-ddThh:mm:ssZ`. \n",
    "\n",
    "The timestamp matches the value used for the [`created_at`](https://cmr.earthdata.nasa.gov/search/site/docs/search/api.html#g-created-at) parameter in the last successful run. This parameter finds the granules created within a range of datetimes. This workflow leverages the `created_at` parameter to search backwards in time for new granules ingested between the time of our timestamp and now.\n",
    "\n",
    "The variables in the cell below determine the workflow behavior on its initial run:\n",
    "\n",
    "* `mins`: Initialize a new local time series by starting with the granules ingested since ___ minutes ago. \n",
    "* `cmr`: The domain of the target CMR instance, either `cmr.earthdata.nasa.gov` or `cmr.uat.earthdata.nasa.gov`.\n",
    "* `ccid`: The unique CMR `concept-id` of the desired collection.\n",
    "* `data`: The path to a local directory in which to download/maintain a copy of the NRT granule time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-11T17:34:08.712987Z",
     "start_time": "2020-08-11T17:34:08.710091Z"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "#\n",
    "# This cell accepts parameters from command line with papermill: \n",
    "#  https://papermill.readthedocs.io\n",
    "#\n",
    "# These variables should be set before the first run, then they \n",
    "#  should be left alone. All subsequent runs expect the values \n",
    "#  for cmr, ccid, data to be unchanged. The mins value has no \n",
    "#  impact on subsequent runs.\n",
    "#\n",
    "\n",
    "mins = 20\n",
    "\n",
    "cmr = \"cmr.uat.earthdata.nasa.gov\"\n",
    "\n",
    "ccid = \"C1234724470-POCLOUD\"\n",
    "\n",
    "data = \"resources/nrt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable `data` is pointed at a nearby folder [`resources/nrt`](resources/nrt/) by default. **You should change `data` to a suitable download path on your file system.** An unlucky sequence of git commands could disappear that folder and its downloads if your not careful. Just change it.\n",
    "\n",
    "The Python imports relevant to the workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-11T17:34:09.343381Z",
     "start_time": "2020-08-11T17:34:09.340343Z"
    }
   },
   "outputs": [],
   "source": [
    "from os import makedirs\n",
    "from os.path import isdir, basename\n",
    "from urllib.parse import urlencode\n",
    "from urllib.request import urlopen, urlretrieve\n",
    "from datetime import datetime, timedelta\n",
    "from json import dumps, loads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The search retrieves granules ingested during the last `n` minutes.** A file in your local data dir  file that tracks updates to your data directory, if one file exists. The CMR Search falls back on the ten minute window if not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-11T17:34:10.190280Z",
     "start_time": "2020-08-11T17:34:10.180693Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2020-08-11T17:14:10Z'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timestamp = (datetime.utcnow()-timedelta(minutes=mins)).strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell will replace the timestamp above with the one read from the `.update` file in the data directory, if it exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-11T17:34:13.477396Z",
     "start_time": "2020-08-11T17:34:13.472753Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: .update found in the data directory. (The last run was at 2020-08-11T17:29:35Z.)\n"
     ]
    }
   ],
   "source": [
    "if not isdir(data):\n",
    "    print(f\"NOTE: Making new data directory at '{data}'. (This is the first run.)\")\n",
    "    makedirs(data)\n",
    "else:\n",
    "    try:\n",
    "        with open(f\"{data}/.update\", \"r\") as f:\n",
    "            timestamp = f.read()\n",
    "    except FileNotFoundError:\n",
    "        print(\"WARN: No .update in the data directory. (Is this the first run?)\")\n",
    "    else:\n",
    "        print(f\"NOTE: .update found in the data directory. (The last run was at {timestamp}.)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several ways to query for CMR updates that occured during a given timeframe. Read on in the CMR Search documentation:\n",
    "\n",
    "* https://cmr.earthdata.nasa.gov/search/site/docs/search/api.html#c-with-new-granules (Collections)\n",
    "* https://cmr.earthdata.nasa.gov/search/site/docs/search/api.html#c-with-revised-granules (Collections)\n",
    "* https://cmr.earthdata.nasa.gov/search/site/docs/search/api.html#g-production-date (Granules)\n",
    "* https://cmr.earthdata.nasa.gov/search/site/docs/search/api.html#g-created-at (Granules)\n",
    "\n",
    "The `created_at` parameter works for our purposes. It's a granule search parameter that returns the records ingested since the input timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-11T17:34:15.176292Z",
     "start_time": "2020-08-11T17:34:15.171348Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'scroll': 'true',\n",
       " 'page_size': 2000,\n",
       " 'sort_key': '-start_date',\n",
       " 'collection_concept_id': 'C1234724470-POCLOUD',\n",
       " 'created_at': '2020-08-11T17:29:35Z',\n",
       " 'bounding_box': '-146.5,57.5,-146,58'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {\n",
    "    'scroll': \"true\",\n",
    "    'page_size': 2000,\n",
    "    'sort_key': \"-start_date\",\n",
    "    'collection_concept_id': ccid, \n",
    "    'created_at': timestamp,\n",
    "    # Limit results to coverage for .5deg bbox in Gulf of Alaska:\n",
    "    'bounding_box': \"-146.5,57.5,-146,58\",\n",
    "}\n",
    "\n",
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the query parameters as a string and then the complete search url:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-11T17:34:16.816986Z",
     "start_time": "2020-08-11T17:34:16.813631Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://cmr.uat.earthdata.nasa.gov/search/granules.umm_json?scroll=true&page_size=2000&sort_key=-start_date&collection_concept_id=C1234724470-POCLOUD&created_at=2020-08-11T17%3A29%3A35Z&bounding_box=-146.5%2C57.5%2C-146%2C58\n"
     ]
    }
   ],
   "source": [
    "query = urlencode(params)\n",
    "url = f\"https://{cmr}/search/granules.umm_json?{query}\"\n",
    "print(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a new timestamp that represents the UTC time of the search. Then download the records in `umm_json` format for granules that match our search parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-11T17:34:18.168810Z",
     "start_time": "2020-08-11T17:34:17.709156Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 new granules ingested for 'C1234724470-POCLOUD' since '2020-08-11T17:34:17Z'.\n"
     ]
    }
   ],
   "source": [
    "timestamp = datetime.utcnow().strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "\n",
    "with urlopen(url) as f:\n",
    "    results = loads(f.read().decode())\n",
    "\n",
    "print(f\"{results['hits']} new granules ingested for '{ccid}' since '{timestamp}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neatly print the first granule record (if one was returned):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-11T17:34:19.667755Z",
     "start_time": "2020-08-11T17:34:19.665059Z"
    }
   },
   "outputs": [],
   "source": [
    "if len(results['items'])>0:\n",
    "    print(dumps(results['items'][0], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The link for http access can be retrieved from each granule record's `RelatedUrls` field. The download link is identified by `\"Type\": \"GET DATA\"` .\n",
    "\n",
    "Select the download URL for each of the granule records:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-11T17:34:20.408069Z",
     "start_time": "2020-08-11T17:34:20.404068Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "downloads = [[u['URL'] for u in r['umm']['RelatedUrls'] if u['Type']==\"GET DATA\"][0] for r in results['items']]\n",
    "downloads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T12:03:56.780074Z",
     "start_time": "2020-07-25T12:03:56.777273Z"
    }
   },
   "source": [
    "Finish by downloading the files to the data directory in a loop. Overwrite `.update` with a new timestamp on success."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-11T17:34:21.088917Z",
     "start_time": "2020-08-11T17:34:21.085442Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for f in downloads:\n",
    "    try:\n",
    "        urlretrieve(f, f\"{data}/{basename(f)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[{datetime.now()}] FAILURE: {f}\\n\\n{e}\\n\")\n",
    "        raise e\n",
    "    else:\n",
    "        print(f\"[{datetime.now()}] SUCCESS: {f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there were updates to the local time series during this run and no exceptions were raised during the download loop, then overwrite the timestamp file that tracks updates to the data folder (`resources/nrt/.update`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-11T17:34:22.362167Z",
     "start_time": "2020-08-11T17:34:22.359386Z"
    }
   },
   "outputs": [],
   "source": [
    "if len(results['items'])>0:\n",
    "    with open(f\"{data}/.update\", \"w\") as f:\n",
    "        f.write(timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "241.528px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
