{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> From the PO.DAAC Cookbook, to access the GitHub version of the notebook, follow [this link](https://github.com/podaac/tutorials/blob/master/notebooks/GIS/SWOTshp_CSVconversion.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# SWOT Shapefile Data Conversion to CSV\n",
    "\n",
    "### Notebook showcasing how to merge/concatenate multiple shapefiles into a single file.\n",
    "- Utilizing the merged shapefile and converting it to a csv file.\n",
    "- Option to query the new dataset based on users choice; either 'reach_id' or water surface elevation ('wse'), etc.\n",
    "- Using the queried variable to export it as a csv or shapefile.\n",
    "\n",
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import os\n",
    "import zipfile\n",
    "import earthaccess\n",
    "from earthaccess import Auth, DataCollections, DataGranules, Store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before you start\n",
    "\n",
    "Before you beginning this tutorial, make sure you have an account in the Earthdata Login, which is required to access data from the NASA Earthdata system. Please visit https://urs.earthdata.nasa.gov to register for an Earthdata Login account. It is free to create and only takes a moment to set up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are already authenticated with NASA EDL\n"
     ]
    }
   ],
   "source": [
    "auth = earthaccess.login() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search for SWOT data\n",
    "Let's start our search for River Vector Shapefiles with a particular pass, pass 013. SWOT files come in \"reach\" and \"node\" versions in the same collection, here we want the 10km reaches rather than the nodes. We will also only get files for North America, or 'NA' and call out a specific pass number that we want. Each dataset has it's own shortname associate with it, for the SWOT River shapefiles, it is SWOT_L2_HR_RiverSP_2.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Granules found: 0\n",
      "Actual granules returned: 5\n"
     ]
    }
   ],
   "source": [
    "results = earthaccess.search_data(short_name = 'SWOT_L2_HR_RIVERSP_2.0', \n",
    "                                  #temporal = ('2023-04-08 00:00:00', '2023-04-25 23:59:59'), # can also specify by time\n",
    "                                  granule_name = '*Reach*_013_NA*', # here we filter by Reach files (not node), pass=013, continent code=NA\n",
    "                                  count = 2000) # necessary to specify count while the dataset is not fully public\n",
    "\n",
    "#this portion of the code chunk below is only necessary while the dataset is not fully public\n",
    "if len(results) >0:\n",
    "    print(f\"Actual granules returned: {len(results)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the science orbit, a pass will be repeated once every 21 days. A particular location may have different passes observe it within the 21 days, however. See the [SWOT swath visualizer](https://swot.jpl.nasa.gov/mission/swath-visualizer/) for your location!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the Data into a folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Getting 5 granules, approx download size: 0.0 GB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2ad0649087140389960d63b2abe8830",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QUEUEING TASKS | :   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "024fd8f668254eaf9a8183551b406612",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "PROCESSING TASKS | :   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "550399482e0f42e1a687c211488e9f39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "COLLECTING RESULTS | :   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "earthaccess.download(results, \"../datasets/data_downloads/SWOT_files\")\n",
    "folder = Path(\"../datasets/data_downloads/SWOT_files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unzip shapefiles in existing folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in os.listdir(folder): # loop through items in dir\n",
    "    if item.endswith(\".zip\"): # check for \".zip\" extension\n",
    "        zip_ref = zipfile.ZipFile(f\"{folder}/{item}\") # create zipfile object\n",
    "        zip_ref.extractall(folder) # extract file to dir\n",
    "        zip_ref.close() # close file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opening multiple shapefiles from within a folder\n",
    "Lets open all the shapefiles we've downloaded together into one database. This approach is ideal for a small number of granules, but if you're looking to create large timeseries, consider using the PO.DAAC Hydrocron tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize list of shapefiles containing all dates\n",
    "SWOT_HR_shps = []\n",
    "\n",
    "# Loop through queried granules to stack all acquisition dates\n",
    "for j in range(len(results)):\n",
    "    filename = earthaccess.results.DataGranule.data_links(results[j], access='external')\n",
    "    filename = filename[0].split(\"/\")[-1]\n",
    "    filename_shp = filename.replace('.zip','.shp')\n",
    "    filename_shp_path = f\"{folder}\\{filename_shp}\"\n",
    "    SWOT_HR_shps.append(gpd.read_file(filename_shp_path)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reach_id</th>\n",
       "      <th>time</th>\n",
       "      <th>time_tai</th>\n",
       "      <th>time_str</th>\n",
       "      <th>p_lat</th>\n",
       "      <th>p_lon</th>\n",
       "      <th>river_name</th>\n",
       "      <th>wse</th>\n",
       "      <th>wse_u</th>\n",
       "      <th>wse_r_u</th>\n",
       "      <th>...</th>\n",
       "      <th>p_wid_var</th>\n",
       "      <th>p_n_nodes</th>\n",
       "      <th>p_dist_out</th>\n",
       "      <th>p_length</th>\n",
       "      <th>p_maf</th>\n",
       "      <th>p_dam_id</th>\n",
       "      <th>p_n_ch_max</th>\n",
       "      <th>p_n_ch_mod</th>\n",
       "      <th>p_low_slp</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 127 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty GeoDataFrame\n",
       "Columns: [reach_id, time, time_tai, time_str, p_lat, p_lon, river_name, wse, wse_u, wse_r_u, wse_c, wse_c_u, slope, slope_u, slope_r_u, slope2, slope2_u, slope2_r_u, width, width_u, width_c, width_c_u, area_total, area_tot_u, area_detct, area_det_u, area_wse, d_x_area, d_x_area_u, layovr_val, node_dist, loc_offset, xtrk_dist, dschg_c, dschg_c_u, dschg_csf, dschg_c_q, dschg_gc, dschg_gc_u, dschg_gcsf, dschg_gc_q, dschg_m, dschg_m_u, dschg_msf, dschg_m_q, dschg_gm, dschg_gm_u, dschg_gmsf, dschg_gm_q, dschg_b, dschg_b_u, dschg_bsf, dschg_b_q, dschg_gb, dschg_gb_u, dschg_gbsf, dschg_gb_q, dschg_h, dschg_h_u, dschg_hsf, dschg_h_q, dschg_gh, dschg_gh_u, dschg_ghsf, dschg_gh_q, dschg_o, dschg_o_u, dschg_osf, dschg_o_q, dschg_go, dschg_go_u, dschg_gosf, dschg_go_q, dschg_s, dschg_s_u, dschg_ssf, dschg_s_q, dschg_gs, dschg_gs_u, dschg_gssf, dschg_gs_q, dschg_i, dschg_i_u, dschg_isf, dschg_i_q, dschg_gi, dschg_gi_u, dschg_gisf, dschg_gi_q, dschg_q_b, dschg_gq_b, reach_q, reach_q_b, dark_frac, ice_clim_f, ice_dyn_f, partial_f, n_good_nod, obs_frac_n, xovr_cal_q, ...]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 127 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine granules from all acquisition dates into one dataframe\n",
    "SWOT_HR_df = gpd.GeoDataFrame(pd.concat(SWOT_HR_shps, ignore_index=True))\n",
    "\n",
    "# Sort dataframe by reach_id and time\n",
    "SWOT_HR_df = SWOT_HR_df.sort_values(['reach_id', 'time'])\n",
    "\n",
    "SWOT_HR_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying a Shapefile\n",
    "\n",
    "Let's get the attributes from a particular reach of the merged shapefile. If you want to search for a specific reach id or a specific length of river reach that is possible through a spatial query using Geopandas. Here, we'll look at a river reach on Cook Slough in Oregon, ID: 78310700041. River IDs can be identified in the [SWORD Database](https://www.swordexplorer.com/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reach_id</th>\n",
       "      <th>time</th>\n",
       "      <th>time_tai</th>\n",
       "      <th>time_str</th>\n",
       "      <th>p_lat</th>\n",
       "      <th>p_lon</th>\n",
       "      <th>river_name</th>\n",
       "      <th>wse</th>\n",
       "      <th>wse_u</th>\n",
       "      <th>wse_r_u</th>\n",
       "      <th>...</th>\n",
       "      <th>p_wid_var</th>\n",
       "      <th>p_n_nodes</th>\n",
       "      <th>p_dist_out</th>\n",
       "      <th>p_length</th>\n",
       "      <th>p_maf</th>\n",
       "      <th>p_dam_id</th>\n",
       "      <th>p_n_ch_max</th>\n",
       "      <th>p_n_ch_mod</th>\n",
       "      <th>p_low_slp</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 127 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty GeoDataFrame\n",
       "Columns: [reach_id, time, time_tai, time_str, p_lat, p_lon, river_name, wse, wse_u, wse_r_u, wse_c, wse_c_u, slope, slope_u, slope_r_u, slope2, slope2_u, slope2_r_u, width, width_u, width_c, width_c_u, area_total, area_tot_u, area_detct, area_det_u, area_wse, d_x_area, d_x_area_u, layovr_val, node_dist, loc_offset, xtrk_dist, dschg_c, dschg_c_u, dschg_csf, dschg_c_q, dschg_gc, dschg_gc_u, dschg_gcsf, dschg_gc_q, dschg_m, dschg_m_u, dschg_msf, dschg_m_q, dschg_gm, dschg_gm_u, dschg_gmsf, dschg_gm_q, dschg_b, dschg_b_u, dschg_bsf, dschg_b_q, dschg_gb, dschg_gb_u, dschg_gbsf, dschg_gb_q, dschg_h, dschg_h_u, dschg_hsf, dschg_h_q, dschg_gh, dschg_gh_u, dschg_ghsf, dschg_gh_q, dschg_o, dschg_o_u, dschg_osf, dschg_o_q, dschg_go, dschg_go_u, dschg_gosf, dschg_go_q, dschg_s, dschg_s_u, dschg_ssf, dschg_s_q, dschg_gs, dschg_gs_u, dschg_gssf, dschg_gs_q, dschg_i, dschg_i_u, dschg_isf, dschg_i_q, dschg_gi, dschg_gi_u, dschg_gisf, dschg_gi_q, dschg_q_b, dschg_gq_b, reach_q, reach_q_b, dark_frac, ice_clim_f, ice_dyn_f, partial_f, n_good_nod, obs_frac_n, xovr_cal_q, ...]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 127 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reach = gdf.query(\"reach_id == '78310700041'\")\n",
    "reach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Converting to CSV\n",
    "\n",
    "We can convert the merged timeseries geodataframe for this reach into a csv file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.to_csv(folder / 'csv_78310700041.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "c5773edd8cfec1b980765f45592751359f797d5bc2e0f18319f112bb9ed701f9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
